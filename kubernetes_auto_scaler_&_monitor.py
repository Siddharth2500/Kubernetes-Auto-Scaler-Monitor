# -*- coding: utf-8 -*-
"""Kubernetes Auto-Scaler & Monitor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y1sEyvWd3UiDWmKNrdVAn4E3hqiXSSDK
"""

# k8s_autoscaler.py
# Standalone Kubernetes Auto-Scaler & Monitor
# No external dependencies required - uses Python standard library only

import json
import logging
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from enum import Enum
import random
from time import sleep
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PodPhase(Enum):
    PENDING = "Pending"
    RUNNING = "Running"
    SUCCEEDED = "Succeeded"
    FAILED = "Failed"
    UNKNOWN = "Unknown"

@dataclass
class PodMetrics:
    name: str
    namespace: str
    deployment: str
    cpu_usage_millicores: float
    memory_usage_mb: float
    cpu_limit_millicores: float
    memory_limit_mb: float
    status: str
    node: str
    timestamp: datetime

    def get_cpu_percentage(self) -> float:
        if self.cpu_limit_millicores > 0:
            return (self.cpu_usage_millicores / self.cpu_limit_millicores) * 100
        return 0.0

    def get_memory_percentage(self) -> float:
        if self.memory_limit_mb > 0:
            return (self.memory_usage_mb / self.memory_limit_mb) * 100
        return 0.0

@dataclass
class DeploymentInfo:
    name: str
    namespace: str
    replicas: int
    available_replicas: int
    ready_replicas: int
    labels: Dict[str, str]
    created_at: datetime

@dataclass
class NodeInfo:
    name: str
    cpu_capacity_cores: float
    memory_capacity_gb: float
    cpu_allocatable_cores: float
    memory_allocatable_gb: float
    pod_count: int
    status: str

@dataclass
class ScalingDecision:
    deployment: str
    namespace: str
    current_replicas: int
    target_replicas: int
    reason: str
    cpu_usage_percent: float
    memory_usage_percent: float
    timestamp: datetime

@dataclass
class ScalingEvent:
    id: str
    deployment: str
    namespace: str
    from_replicas: int
    to_replicas: int
    reason: str
    status: str
    timestamp: datetime

# Mock Kubernetes API for standalone operation
class MockKubernetesAPI:
    """Simulates Kubernetes API responses"""

    def __init__(self):
        self.deployments = self._generate_mock_deployments()
        self.pods = {}
        self.nodes = self._generate_mock_nodes()

    def _generate_mock_deployments(self) -> Dict[str, DeploymentInfo]:
        deployment_names = ['web-app', 'api-server', 'worker-service', 'cache-service', 'auth-service']
        deployments = {}

        for name in deployment_names:
            replicas = random.randint(2, 5)
            deployments[name] = DeploymentInfo(
                name=name,
                namespace='default',
                replicas=replicas,
                available_replicas=replicas,
                ready_replicas=replicas,
                labels={'app': name, 'tier': random.choice(['frontend', 'backend', 'middleware'])},
                created_at=datetime.now() - timedelta(days=random.randint(1, 90))
            )

        return deployments

    def _generate_mock_nodes(self) -> List[NodeInfo]:
        nodes = []
        for i in range(3):
            nodes.append(NodeInfo(
                name=f'node-{i+1}',
                cpu_capacity_cores=4.0,
                memory_capacity_gb=16.0,
                cpu_allocatable_cores=3.8,
                memory_allocatable_gb=14.5,
                pod_count=random.randint(10, 30),
                status='Ready'
            ))
        return nodes

    def generate_pod_metrics(self, deployment_name: str, namespace: str, replica_count: int) -> List[PodMetrics]:
        """Generate realistic pod metrics"""
        metrics = []

        for i in range(replica_count):
            # Simulate varying load patterns
            base_cpu = random.uniform(100, 800)  # millicores
            base_memory = random.uniform(256, 2048)  # MB

            # Add some variance to simulate real workload
            cpu_usage = base_cpu + random.uniform(-50, 150)
            memory_usage = base_memory + random.uniform(-100, 200)

            metrics.append(PodMetrics(
                name=f'{deployment_name}-{random.randint(10000, 99999)}-{random.randint(100, 999)}',
                namespace=namespace,
                deployment=deployment_name,
                cpu_usage_millicores=max(50, cpu_usage),
                memory_usage_mb=max(128, memory_usage),
                cpu_limit_millicores=1000.0,  # 1 CPU
                memory_limit_mb=2048.0,  # 2GB
                status='Running',
                node=random.choice([n.name for n in self.nodes]),
                timestamp=datetime.now()
            ))

        return metrics

class KubernetesMonitor:
    """Monitors Kubernetes cluster resources and metrics"""

    def __init__(self, mock_mode: bool = True):
        self.mock_mode = mock_mode
        self.mock_api = MockKubernetesAPI() if mock_mode else None
        logger.info("Kubernetes Monitor initialized (Mock Mode: %s)", mock_mode)

    def get_pod_metrics(self, namespace: str = 'default', deployment: Optional[str] = None) -> List[PodMetrics]:
        """Get pod metrics for a namespace or specific deployment"""
        if not self.mock_mode:
            logger.error("Real Kubernetes API not implemented. Use mock_mode=True")
            return []

        all_metrics = []

        if deployment:
            if deployment in self.mock_api.deployments:
                dep_info = self.mock_api.deployments[deployment]
                metrics = self.mock_api.generate_pod_metrics(deployment, namespace, dep_info.replicas)
                all_metrics.extend(metrics)
        else:
            for dep_name, dep_info in self.mock_api.deployments.items():
                if dep_info.namespace == namespace:
                    metrics = self.mock_api.generate_pod_metrics(dep_name, namespace, dep_info.replicas)
                    all_metrics.extend(metrics)

        logger.info("Retrieved metrics for %d pods", len(all_metrics))
        return all_metrics

    def get_deployment_info(self, name: str, namespace: str = 'default') -> Optional[DeploymentInfo]:
        """Get deployment information"""
        if not self.mock_mode:
            return None

        if name in self.mock_api.deployments:
            return self.mock_api.deployments[name]
        return None

    def list_deployments(self, namespace: str = 'default') -> List[str]:
        """List all deployments in a namespace"""
        if not self.mock_mode:
            return []

        return [name for name, info in self.mock_api.deployments.items()
                if info.namespace == namespace]

    def scale_deployment(self, name: str, replicas: int, namespace: str = 'default') -> bool:
        """Scale a deployment to specified replicas"""
        if not self.mock_mode:
            return False

        if name in self.mock_api.deployments:
            old_replicas = self.mock_api.deployments[name].replicas
            self.mock_api.deployments[name].replicas = replicas
            self.mock_api.deployments[name].available_replicas = replicas
            self.mock_api.deployments[name].ready_replicas = replicas

            logger.info("Scaled deployment %s from %d to %d replicas", name, old_replicas, replicas)
            return True

        logger.error("Deployment %s not found", name)
        return False

    def get_cluster_resources(self) -> Dict:
        """Get cluster-wide resource information"""
        if not self.mock_mode:
            return {}

        total_cpu = sum(node.cpu_capacity_cores for node in self.mock_api.nodes)
        total_memory = sum(node.memory_capacity_gb for node in self.mock_api.nodes)
        allocatable_cpu = sum(node.cpu_allocatable_cores for node in self.mock_api.nodes)
        allocatable_memory = sum(node.memory_allocatable_gb for node in self.mock_api.nodes)

        return {
            'node_count': len(self.mock_api.nodes),
            'total_cpu_cores': total_cpu,
            'total_memory_gb': total_memory,
            'allocatable_cpu_cores': allocatable_cpu,
            'allocatable_memory_gb': allocatable_memory,
            'nodes': [
                {
                    'name': node.name,
                    'cpu_cores': node.cpu_capacity_cores,
                    'memory_gb': node.memory_capacity_gb,
                    'pod_count': node.pod_count,
                    'status': node.status
                }
                for node in self.mock_api.nodes
            ]
        }

    def get_node_metrics(self) -> List[NodeInfo]:
        """Get metrics for all nodes"""
        if not self.mock_mode:
            return []
        return self.mock_api.nodes

class AutoScaler:
    """Intelligent Kubernetes auto-scaler"""

    def __init__(self,
                 monitor: KubernetesMonitor,
                 cpu_threshold_up: float = 70.0,
                 cpu_threshold_down: float = 30.0,
                 memory_threshold_up: float = 80.0,
                 memory_threshold_down: float = 40.0,
                 min_replicas: int = 2,
                 max_replicas: int = 10,
                 cooldown_minutes: int = 5):

        self.monitor = monitor
        self.cpu_threshold_up = cpu_threshold_up
        self.cpu_threshold_down = cpu_threshold_down
        self.memory_threshold_up = memory_threshold_up
        self.memory_threshold_down = memory_threshold_down
        self.min_replicas = min_replicas
        self.max_replicas = max_replicas
        self.cooldown_period = timedelta(minutes=cooldown_minutes)

        self.scaling_history: List[ScalingEvent] = []
        self.last_scaling: Dict[str, datetime] = {}

        logger.info("AutoScaler initialized with CPU thresholds: %.1f%%/%.1f%%, Memory: %.1f%%/%.1f%%",
                   cpu_threshold_up, cpu_threshold_down, memory_threshold_up, memory_threshold_down)

    def evaluate_deployment(self, deployment: str, namespace: str = 'default') -> Optional[ScalingDecision]:
        """Evaluate if a deployment needs scaling"""

        # Get current deployment info
        dep_info = self.monitor.get_deployment_info(deployment, namespace)
        if not dep_info:
            logger.warning("Deployment %s not found", deployment)
            return None

        current_replicas = dep_info.replicas

        # Check cooldown period
        key = f"{namespace}/{deployment}"
        if key in self.last_scaling:
            time_since_last = datetime.now() - self.last_scaling[key]
            if time_since_last < self.cooldown_period:
                logger.debug("Deployment %s in cooldown period (%.1f minutes remaining)",
                           deployment, (self.cooldown_period - time_since_last).total_seconds() / 60)
                return None

        # Get pod metrics
        pod_metrics = self.monitor.get_pod_metrics(namespace, deployment)
        if not pod_metrics:
            logger.warning("No metrics available for deployment %s", deployment)
            return None

        # Calculate average resource usage
        avg_cpu = sum(m.get_cpu_percentage() for m in pod_metrics) / len(pod_metrics)
        avg_memory = sum(m.get_memory_percentage() for m in pod_metrics) / len(pod_metrics)

        logger.debug("Deployment %s - Avg CPU: %.1f%%, Avg Memory: %.1f%%",
                    deployment, avg_cpu, avg_memory)

        # Determine scaling action
        target_replicas = current_replicas
        reason = "No scaling needed"

        # Scale up conditions
        if avg_cpu > self.cpu_threshold_up or avg_memory > self.memory_threshold_up:
            target_replicas = min(current_replicas + 1, self.max_replicas)
            if avg_cpu > self.cpu_threshold_up:
                reason = f"High CPU usage: {avg_cpu:.1f}% > {self.cpu_threshold_up}%"
            else:
                reason = f"High memory usage: {avg_memory:.1f}% > {self.memory_threshold_up}%"

        # Scale down conditions
        elif avg_cpu < self.cpu_threshold_down and avg_memory < self.memory_threshold_down:
            if current_replicas > self.min_replicas:
                target_replicas = max(current_replicas - 1, self.min_replicas)
                reason = f"Low resource usage - CPU: {avg_cpu:.1f}%, Memory: {avg_memory:.1f}%"

        # Create scaling decision if needed
        if target_replicas != current_replicas:
            return ScalingDecision(
                deployment=deployment,
                namespace=namespace,
                current_replicas=current_replicas,
                target_replicas=target_replicas,
                reason=reason,
                cpu_usage_percent=avg_cpu,
                memory_usage_percent=avg_memory,
                timestamp=datetime.now()
            )

        return None

    def apply_scaling_decision(self, decision: ScalingDecision) -> bool:
        """Apply a scaling decision"""
        success = self.monitor.scale_deployment(
            decision.deployment,
            decision.target_replicas,
            decision.namespace
        )

        if success:
            # Record scaling event
            event = ScalingEvent(
                id=f"scale-{len(self.scaling_history) + 1}",
                deployment=decision.deployment,
                namespace=decision.namespace,
                from_replicas=decision.current_replicas,
                to_replicas=decision.target_replicas,
                reason=decision.reason,
                status='completed',
                timestamp=decision.timestamp
            )

            self.scaling_history.append(event)

            # Update last scaling time
            key = f"{decision.namespace}/{decision.deployment}"
            self.last_scaling[key] = decision.timestamp

            logger.info("âœ“ Scaling applied: %s from %d to %d replicas - %s",
                       decision.deployment, decision.current_replicas,
                       decision.target_replicas, decision.reason)
            return True

        return False

    def evaluate_all_deployments(self, namespace: str = 'default') -> List[ScalingDecision]:
        """Evaluate all deployments and return scaling decisions"""
        decisions = []
        deployments = self.monitor.list_deployments(namespace)

        logger.info("Evaluating %d deployments in namespace %s", len(deployments), namespace)

        for deployment in deployments:
            decision = self.evaluate_deployment(deployment, namespace)
            if decision:
                decisions.append(decision)

        return decisions

    def run_scaling_cycle(self, namespace: str = 'default') -> int:
        """Run one scaling cycle and apply all decisions"""
        decisions = self.evaluate_all_deployments(namespace)

        applied_count = 0
        for decision in decisions:
            if self.apply_scaling_decision(decision):
                applied_count += 1

        if applied_count > 0:
            logger.info("Scaling cycle complete: %d actions applied", applied_count)

        return applied_count

    def run_continuous_monitoring(self, interval_seconds: int = 60,
                                  namespace: str = 'default',
                                  max_cycles: Optional[int] = None):
        """Run continuous monitoring and scaling"""
        logger.info("Starting continuous monitoring (interval: %ds, namespace: %s)",
                   interval_seconds, namespace)

        cycle_count = 0

        try:
            while True:
                cycle_count += 1
                logger.info("=" * 60)
                logger.info("Monitoring Cycle #%d", cycle_count)
                logger.info("=" * 60)

                actions_taken = self.run_scaling_cycle(namespace)

                if max_cycles and cycle_count >= max_cycles:
                    logger.info("Reached maximum cycles (%d), stopping", max_cycles)
                    break

                logger.info("Waiting %d seconds until next cycle...", interval_seconds)
                sleep(interval_seconds)

        except KeyboardInterrupt:
            logger.info("Monitoring stopped by user")

    def get_scaling_report(self) -> Dict:
        """Generate scaling report"""
        total_events = len(self.scaling_history)

        scale_up_events = [e for e in self.scaling_history if e.to_replicas > e.from_replicas]
        scale_down_events = [e for e in self.scaling_history if e.to_replicas < e.from_replicas]

        deployments_scaled = len(set(e.deployment for e in self.scaling_history))

        return {
            'total_scaling_events': total_events,
            'scale_up_events': len(scale_up_events),
            'scale_down_events': len(scale_down_events),
            'deployments_scaled': deployments_scaled,
            'recent_events': [
                {
                    'id': e.id,
                    'deployment': e.deployment,
                    'namespace': e.namespace,
                    'from_replicas': e.from_replicas,
                    'to_replicas': e.to_replicas,
                    'action': 'scale_up' if e.to_replicas > e.from_replicas else 'scale_down',
                    'reason': e.reason,
                    'timestamp': e.timestamp.isoformat(),
                    'status': e.status
                }
                for e in self.scaling_history[-10:]
            ]
        }

    def export_scaling_history(self, filename: str):
        """Export scaling history to JSON"""
        history = [
            {
                'id': e.id,
                'deployment': e.deployment,
                'namespace': e.namespace,
                'from_replicas': e.from_replicas,
                'to_replicas': e.to_replicas,
                'reason': e.reason,
                'status': e.status,
                'timestamp': e.timestamp.isoformat()
            }
            for e in self.scaling_history
        ]

        with open(filename, 'w') as f:
            json.dump(history, f, indent=2)

        logger.info("Scaling history exported to %s", filename)

def main():
    """Main execution function"""
    print("=" * 70)
    print(" Kubernetes Auto-Scaler & Monitor - Standalone Demo")
    print("=" * 70)
    print()

    # Initialize monitor and autoscaler
    monitor = KubernetesMonitor(mock_mode=True)

    # Display cluster information
    print("ðŸ“Š Cluster Information:")
    cluster_info = monitor.get_cluster_resources()
    print(f"  Nodes: {cluster_info['node_count']}")
    print(f"  Total CPU: {cluster_info['total_cpu_cores']} cores")
    print(f"  Total Memory: {cluster_info['total_memory_gb']:.1f} GB")
    print()

    # List deployments
    deployments = monitor.list_deployments('default')
    print(f"ðŸ“¦ Deployments in 'default' namespace: {len(deployments)}")
    for dep_name in deployments:
        dep_info = monitor.get_deployment_info(dep_name)
        print(f"  - {dep_name}: {dep_info.replicas} replicas")
    print()

    # Initialize autoscaler
    autoscaler = AutoScaler(
        monitor=monitor,
        cpu_threshold_up=70.0,
        cpu_threshold_down=30.0,
        memory_threshold_up=80.0,
        memory_threshold_down=40.0,
        min_replicas=2,
        max_replicas=10,
        cooldown_minutes=5
    )

    # Run 3 monitoring cycles
    print("ðŸ”„ Running monitoring cycles...")
    print("=" * 70)
    autoscaler.run_continuous_monitoring(interval_seconds=5, namespace='default', max_cycles=3)

    # Generate report
    print()
    print("=" * 70)
    print(" Scaling Report")
    print("=" * 70)
    report = autoscaler.get_scaling_report()
    print(json.dumps(report, indent=2))

    # Export history
    print()
    print("=" * 70)
    print(" Exporting Data")
    print("=" * 70)
    autoscaler.export_scaling_history('scaling_history.json')
    print("âœ“ Scaling history exported to: scaling_history.json")

    print()
    print("=" * 70)
    print(" Demo completed successfully!")
    print("=" * 70)

if __name__ == "__main__":
    main()